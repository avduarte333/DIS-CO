[build-system]
requires = ["setuptools>=61", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "discopyright"
version = "0.1.0"
description = "Core functionality for the movie guessing task using various LLM models."
readme = "README.md"
requires-python = ">=3.8"
license = { file = "LICENSE" }
authors = [
    { name = "Andre V. Duarte", email = "andre.v.duarte@tecnico.ulisboa.pt" }
]

# Base dependencies (needed for any usage of your package)
dependencies = [
    "pandas==2.2.3",        
    "Pillow==10.4.0",       
    "tqdm==4.66.5",         
    "pydantic==2.9.2",      
    "datasets==3.0.1",      
    "httpx==0.27.2",        # Enforce httpx version -> may trigger compatibility issues with OpenAI if installing latest version
    "openpyxl==3.1.5",
]

[project.optional-dependencies]
# Optional extras for different model branches:

# Gemini branch: uses Google's generative AI client.
gemini = [
    "google-genai==1.0.0"
]

# GPT branch: uses OpenAI's client.
gpt = [
    "openai==1.47.1",
]

# Qwen branch: requires Transformers, Torch, and qwen-vl-utils.
qwen = [
    "transformers==4.46.1",
    "torch==2.4.0",         # CUDA Version 11.6
    "qwen-vl-utils==0.0.8",
    "openai==1.47.1", # Required for Cleaning LLM Output
]

# Llama branch: requires Transformers, Torch, and Hugging Face Hub.
llama = [
    "transformers==4.46.1",
    "torch==2.4.0",         # CUDA Version 11.6
    "huggingface-hub==0.25.1",
    "openai==1.47.1", # Required for Cleaning LLM Output
]

# An extra that combines all optional dependencies.
all = [
    "google-genai==1.0.0",
    "transformers==4.46.1",
    "torch==2.4.0",         # CUDA Version 11.6
    "qwen-vl-utils==0.0.8",
    "huggingface-hub==0.25.1",
    "openai==1.47.1", # Required for Cleaning LLM Output
]
